South African researchers have played a key role in developing and running the very powerful computer system, and the subsequent data analysis, that supports the Atlas detector in the Large Hadron Collider (LHC), which is the world’s biggest and most powerful particle accelerator. The LHC lies on the French-Swiss border and was constructed, and is operated by, CERN (the European Organisation for Nuclear Research). Atlas is one of four major detectors in the LHC. “The overall Atlas team is made up of over 3 000 scientists all over the world, who are running searches for new [sub-atomic] particles, processing data and performing computer simulations,” reported University of Johannesburg (UJ) physics researcher Dr Simon Connell. “They also develop all aspects of the detector and computing systems – 24 hours a day, every day of the year.” A team of UJ researchers has been making a continuing contribution to the Atlas high-performance computing cluster, which is the nerve centre for the detector. “A UJ staff member has worked on the Trigger and Data Acquisition (TDAQ) Computer system for the past decade,” he highlighted. “Acquiring the huge amounts of data at Atlas, fast enough to keep up, is pioneering new trends in modern computing,” pointed out UJ Atlas team member Haydn du Plessis. “The challenge is really acute in the nerve centre, a very large TDAQ computing farm.” In particular, Du Plessis has been exploring the use of containerisation and virtualisation in the detector’s computer farm. Containerisation and virtualisation (more strictly, virtual machines) are both means of deploying applications. They both allow for abstraction of the workload from the underlying hardware. But they are different – containerisation is Operating System virtualisation, while virtual machines provide hardware virtualisation. These approaches allow thousands of tasks to be run simultaneously but separately in the computing cluster, making the most efficient use of the computing resources available. Inspired by Google research, he has been developing new techniques. “Containerisation and virtualisation makes the cluster dramatically more reliable and flexible, which is significant for CERN,” he explained. “It is another way of getting more performance out of the same physical infrastructure.” “Researchers trained at CERN transfer expertise to national IT [information technology] systems in the countries they come from,” stressed Connell. The advanced computing techniques developed for the Atlas research consortium have already been shared with UJ and South Africa. It was this powerful data processing capability which allowed the Atlas consortium to observe the decay of a Higgs boson into bottom quarks, a most important observation which validated a new section of the Standard Model of physics. (Quarks are elementary particles which make up particles such as protons and neutrons; there are six varieties, namely up, down, top, bottom, strange and charmed. The Standard Model is the modern theory concerning the structure of the universe at the sub-atomic level.) The discovery of this new decay mode was announced on August 28.